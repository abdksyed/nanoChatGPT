{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScratchConfig:\n",
    "\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    tokenizer_file: str = \"./data/opus_books/tokenizer_{0}.json\"\n",
    "    src_lang: str = \"fr\"\n",
    "    tgt_lang: str = \"en\"\n",
    "    batch_size: int = 16\n",
    "    max_seq_len: int = 350\n",
    "\n",
    "    # Model\n",
    "    d_model: int = 512\n",
    "    num_layers: int = 6\n",
    "    num_heads: int = 8\n",
    "    d_ff: int = 2048\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    weights_folder = \"./weights\"\n",
    "    epochs_save = \"scratch_epoch_{0:03d}.pt\"\n",
    "    log_dir = \"./logs/scratch\"\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 10\n",
    "    lr: float = 1e-4\n",
    "    label_smoothing: float = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Embeddings are basically, just the vector form of a token(word), which are used as representation for that particular token. The embedding capture the semantic and syntatic properties of the words, and also have an emerging property that words with similar meaning will have similar vectors, here similar means the distance to such vectors is less compared to some random vector.\n",
    "\n",
    "These vectors can be of any size, the larger the vector size, the better meaning it can capture, but also consumes more memory. In original transformer the embedding size used was 512, whereas in BERT-base its 768, and in latest GPT models its even larger like 2048, 4096 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Input Embeddings Example](../images/input_emb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor | torch.cuda.FloatTensor) -> torch.FloatTensor | torch.cuda.FloatTensor:\n",
    "        # x: (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
    "\n",
    "        # (from Paper) In the embedding layers, we multiply those weights by âˆšd_model\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 6, 6, 9],\n",
      "        [9, 3, 3, 7, 6],\n",
      "        [7, 5, 4, 7, 8]])\n",
      "tensor([[[-0.5992, -0.1990],\n",
      "         [-0.5992, -0.1990],\n",
      "         [ 0.6258, -0.0069],\n",
      "         [ 0.6258, -0.0069],\n",
      "         [ 0.6561, -1.5271]],\n",
      "\n",
      "        [[ 0.6561, -1.5271],\n",
      "         [-0.2786, -0.2459],\n",
      "         [-0.2786, -0.2459],\n",
      "         [ 1.0389,  0.5644],\n",
      "         [ 0.6258, -0.0069]],\n",
      "\n",
      "        [[ 1.0389,  0.5644],\n",
      "         [-0.6462, -0.5525],\n",
      "         [ 0.4011,  1.6332],\n",
      "         [ 1.0389,  0.5644],\n",
      "         [ 0.1460,  0.1679]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "input_embedding = InputEmbedding(d_model=2, vocab_size=100)\n",
    "x = torch.randint(low=0, high=10, size=(3,5)) # (batch_size, seq_len)\n",
    "print(x)\n",
    "print(input_embedding(x)) # (batch_size, seq_len, d_model)\n",
    "print(input_embedding(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transformers, uses attention, which is \"permutation-equivariant\", which simply means that the if you change the order of the input, the order of output will change in the same, and there won't be any difference in the values apart from order change. The self-attention does weighted sum of all elements, since sum is same irrespective of the order of the sequence, the output will be same.\n",
    "\n",
    "But in language, the order of words does matter. The sequence \"Man cooks Turkey\" and \"Turkey cooks Man\" has totally different meaning, but for self-attention it doesn't matter the input is \"1,2,3\" or \"3,2,1\", and will give same results. But we want the embeddings to be different when the different order is used.\n",
    "\n",
    "Hence we use positional embeddings, where we add the same dimension vector to each token. In original transformer we used fixed embedding, which can be pre-calculate for each position in the sequence and for each embedding vector value.\n",
    "\n",
    "But there are positional embeddings which can be learnt during training, and are set as weights, and there are other positional embeddings like `rotary` embeddings which depends on the relative position of the tokens rather than absolute position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Positional Embeddings](../images/pos_emb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Positional Encoding](../images/pos_enc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Weights of shape (seq_len, d_model)\n",
    "        pe = torch.zeros((self.seq_len, self.d_model))\n",
    "\n",
    "        # Position Vector\n",
    "        position = torch.arange(0, seq_len, dtype=float).unsqueeze(1) # (seq_len, 1)\n",
    "\n",
    "        # Division term using `log` for numerical stability\n",
    "        # It is mathematically equal to the above formula.\n",
    "        # e^(2i * ln(10_000)/d) == 10_000^(2i/d)\n",
    "        # negative sign to make it denominator\n",
    "        div_term = torch.exp( torch.arange(0,d_model,2).float() * (-math.log(10_000.)/self.d_model) ) # (d_model/2)\n",
    "\n",
    "        # sin() for even position and cos() for odd position\n",
    "        # for each token all even position in the position embedding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "\n",
    "        # register positional encoding as a persistent buffer within the module\n",
    "        # automatically tracked by the module and included in the state dictionary\n",
    "        # when saving or loading the model\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor | torch.cuda.FloatTensor) -> torch.FloatTensor | torch.cuda.FloatTensor:\n",
    "        # x: (batch_size, seq_len, d_model)  -> (batch_size, seq_len, d_model)\n",
    "        # Positional Encodings here are fixed, so setting the gradients to False inplace.\n",
    "        # Using x.shape[1], to use PEs from 0 to the size of input sequence.\n",
    "        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False) # (batch_size, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 2])\n",
      "torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 2\n",
    "positional_embedding = PositionalEmbedding(d_model=embedding_dim, seq_len=seq_len, dropout=0.1)\n",
    "x = torch.randn((batch_size,seq_len,embedding_dim))\n",
    "print(x.shape) # (batch_size, seq_len, d_model)\n",
    "print(positional_embedding(x).shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this normalization is to stabilize the network, speed up convergence, and reduce the sensitivity to the initialization of the model parameters. \n",
    "\n",
    "Normalization helps in dealing with the problem of internal covariate shift, where the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This can slow down the training process and make it harder for the network to converge. Normalization mitigates this problem by ensuring that the layer's inputs are more stable.\n",
    "\n",
    "The transformer uses `LayerNormalization` over the `BatchNormalization` as to avoid calculating statistics over the entire batch, which can slow down training, as it has to bring all the batches on single machine to get statistics and send it back to different machines after normalization. Whereas Layer Normalization, normalizes values across each feature in a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 10e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(features)) # multiplied\n",
    "        self.beta = nn.Parameter(torch.zeros(features)) # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        mean = x.mean(dim=-1, keepdim=True) # (batch_size, seq_len, 1)\n",
    "        std = x.std(dim=-1, keepdim=True) # (batch_size, seq_len, 1)\n",
    "\n",
    "        return self.gamma * ( (x - mean) / (std + self.eps) ) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 2])\n",
      "torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 2\n",
    "layer_norm = LayerNormalization(features=embedding_dim, eps=10e-6)\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) # (batch_size, seq_len, d_model)\n",
    "print(x.shape) # (batch_size, seq_len, d_model)\n",
    "print(layer_norm(x).shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla 2 layer fully connected network along with dropout, to transform the representation obtained from the self-attention mechanism, and adding depth to the model.\n",
    "\n",
    "The self-attention mechanism helps the model understand the context of each word in the sequence, the FFN help the model to represent the words by taking account the words individual meaning and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features=d_model, out_features=d_ff, bias=True) # W1, b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(in_features=d_ff, out_features=d_model, bias=True) # W2, b2\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor | torch.cuda.FloatTensor) -> torch.FloatTensor | torch.cuda.FloatTensor:\n",
    "        # x: (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        # (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        return self.linear2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 2])\n",
      "torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 2\n",
    "d_ff = 4\n",
    "\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(x.shape) # (batch_size, seq_len, d_model)\n",
    "print(ffn_block(x).shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention, is a mechanism that allows each token in the input to interact with every other token. It computes a weighted sum of all input tokens' representations for each token, where the weights are determined by the input tokens themselves. In simple words, for each token, it asks all the other tokens `how much related you are to me?`, and the relationship is given by a weight. This relationship is produced by doing simple dot product between two tokens. If two tokens are having high relationship, the attention score between them will be higher and vice-versa.\n",
    "\n",
    "The self-attention mechanism allows the model to focus on different parts of the input sequence when processing each token, which helps it understand the context and dependencies between words in a sentence, even if they are far apart.\n",
    "\n",
    "At the end, it then multiplies this attention-score matrix, with the final `value` embeddings, to acheive each token final value. This final value of the token now contains the relationship/context of itself with all other tokens based on the attention score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi Head Attention](../images/mh_attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float, verbose: bool =False) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads = num_heads # 8\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads # 512//8 = 64\n",
    "        self.w_q = nn.Linear(in_features=d_model, out_features=d_model, bias=True) # Wq\n",
    "        self.w_k = nn.Linear(in_features=d_model, out_features=d_model, bias=True) # Wk\n",
    "        self.w_v = nn.Linear(in_features=d_model, out_features=d_model, bias=True) # Wv\n",
    "\n",
    "        self.w_o = nn.Linear(in_features=num_heads*self.d_k, out_features=d_model, bias=True) # Wo\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(key, query, value, mask, dropout) -> torch.FloatTensor | torch.cuda.FloatTensor:\n",
    "        # (batch_size, num_heads, seq_len, d_k) -> (batch_size, num_heads, seq_len, d_k)\n",
    "        d_k =  query.shape[-1]\n",
    "\n",
    "        attention_scores = query @ key.transpose(-2, -1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores /= math.sqrt(d_k) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        if mask is not None:\n",
    "            # Replace all values in attention scores, where mask is 0, with -1e9\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        attention_scores = dropout(attention_scores) # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, seq_len) @ (batch_size, num_heads, seq_len, d_k)\n",
    "        # -> (batch_size, num_heads, seq_len, d_k)\n",
    "        return attention_scores @ value, attention_scores\n",
    "\n",
    "    def forward(self,\n",
    "                q: torch.FloatTensor | torch.cuda.FloatTensor,\n",
    "                k: torch.FloatTensor | torch.cuda.FloatTensor,\n",
    "                v: torch.FloatTensor | torch.cuda.FloatTensor,\n",
    "                mask: torch.FloatTensor | torch.cuda.FloatTensor,\n",
    "                ) -> torch.FloatTensor | torch.cuda.FloatTensor:\n",
    "        # q: (batch_size, seq_len, d_model)\n",
    "        query = self.w_q(q) # (batch_size, seq_len, d_model)\n",
    "        # k: (batch_size, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch_size, seq_len, d_model)\n",
    "        # v: (batch_size, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch_size, seq_len, d_model)\n",
    "\n",
    "        batch_size, seq_len = query.shape[:2]\n",
    "        # using einops.rearrange: rearrange(query, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        \n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2) # (batch_size, num_heads, seq_len, d_k)\n",
    "        # During Decoder cross-attn, seq_len of query and key/value can be different, hence using `-1` for key/value\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) # (batch_size, num_heads, -1, d_k)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) # (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Query shape\", query.shape)\n",
    "            print(\"Key shape\", key.shape)\n",
    "            print(\"Value shape\", value.shape)\n",
    "\n",
    "        x, attn_scores = MultiHeadAttentionBlock.attention(key, query, value, mask, self.dropout)\n",
    "        # x: (batch_size, num_heads, seq_len, d_k)\n",
    "        if self.verbose:\n",
    "            print(\"Attention Scores shape\", attn_scores.shape)\n",
    "            print(\"Attention Output shape\", x.shape)\n",
    "\n",
    "        # continguous to make the tensor in contiguous block of memory\n",
    "        x = x.transpose(1,2).contiguous() # (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads*self.d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"MHA Final Output shape\", x.shape)\n",
    "\n",
    "        return self.w_o(x) # attn_scores # (batch_size, seq_len, d_model), (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 8])\n",
      "\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "x = attn_block(x,x,x, mask=None)\n",
    "print()\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual/Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features=features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # In recent changes/implementations, norm is performed before sublayer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 2])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 2\n",
    "d_ff = 4\n",
    "\n",
    "res_block = ResidualConnection(features=embedding_dim, dropout=0.1)\n",
    "\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "\n",
    "x = res_block(x, lambda x: FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)(x))\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder Block](../images/encoder.png)\n",
    "\n",
    "[Source](https://kikaben.com/transformers-encoder-decoder/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features: int,\n",
    "                 self_attn_block: MultiHeadAttentionBlock,\n",
    "                 feed_foward_block: FeedForwardBlock,\n",
    "                 dropout: float,\n",
    "                 verbose: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn_block = self_attn_block\n",
    "        self.feed_foward_block = feed_foward_block\n",
    "        # Two Residual Connections, one after MHA and other after FFN\n",
    "        self.skip = nn.ModuleList(\n",
    "            [\n",
    "                ResidualConnection(features=features, dropout=dropout) for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # x: (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        x = self.skip[0](x, lambda x: self.self_attn_block(x, x, x, src_mask)) # x + dropout( MHA( LayerNorm(x) ) )\n",
    "        if self.verbose:\n",
    "            print(\"EncoderBlock MHA Output shape\", x.shape)\n",
    "        x = self.skip[1](x, self.feed_foward_block) # x + dropout( FFN( LayerNorm(x) ) )\n",
    "        if self.verbose:\n",
    "            print(\"EncoderBlock FFN Output shape\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 8])\n",
      "\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock MHA Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "d_ff = 8\n",
    "\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "res_block = ResidualConnection(features=embedding_dim, dropout=0.1)\n",
    "enc_block = EncoderBlock(features=embedding_dim, self_attn_block=attn_block, \n",
    "                         feed_foward_block=ffn_block, dropout=0.1,\n",
    "                         verbose=True)\n",
    "\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "\n",
    "x = enc_block(x, src_mask=None)\n",
    "print()\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features=features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 8])\n",
      "\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock MHA Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock MHA Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "d_ff = 8\n",
    "\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "enc_block = EncoderBlock(features=embedding_dim, self_attn_block=attn_block, \n",
    "                         feed_foward_block=ffn_block, dropout=0.1,\n",
    "                         verbose=True)\n",
    "enc_stack = Encoder(features=embedding_dim, layers=nn.ModuleList([enc_block]*2))\n",
    "\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "\n",
    "x = enc_stack(x, mask=None)\n",
    "print()\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decoder Block](../images/decoder.png)\n",
    "\n",
    "[Source](https://kikaben.com/transformers-encoder-decoder/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int,\n",
    "                 self_attn_block: MultiHeadAttentionBlock,\n",
    "                 cross_attn_block: MultiHeadAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock,\n",
    "                 dropout: float,\n",
    "                 verbose: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn_block = self_attn_block\n",
    "        self.cross_attn_block = cross_attn_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        # Three Residual Connections, one after each block\n",
    "        self.skip = nn.ModuleList(\n",
    "            [\n",
    "                ResidualConnection(features=features, dropout=dropout) for _ in range(3)\n",
    "            ]\n",
    "        )\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        # encoder_output: (batch_size, seq_len, d_model)\n",
    "        # src_mask: (batch_size, seq_len, seq_len)\n",
    "        # tgt_mask: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        x = self.skip[0](x, lambda x: self.self_attn_block(x, x, x, tgt_mask)) # Input + MHA -> LayerNorm -> Dropout\n",
    "        if self.verbose:\n",
    "            print(\"DecoderBlock Self-Attention Output shape\", x.shape)\n",
    "        # Input + MHA -> LayerNorm -> Dropout\n",
    "        x = self.skip[1](x, lambda x: self.cross_attn_block(x, encoder_output, encoder_output, src_mask)) \n",
    "        if self.verbose:\n",
    "            print(\"DecoderBlock Cross-Attention Output shape\", x.shape)\n",
    "        x = self.skip[2](x, self.feed_forward_block) # Input + FFN -> LayerNorm -> Dropout\n",
    "        if self.verbose:\n",
    "            print(\"DecoderBlock FFN Output shape\", x.shape)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 8])\n",
      "\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Self-Attention Output shape torch.Size([3, 5, 8])\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Cross-Attention Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "d_ff = 8\n",
    "\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "cross_attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "\n",
    "dec_block = DecoderBlock(features=embedding_dim, self_attn_block=attn_block,\n",
    "                         cross_attn_block=cross_attn_block, feed_forward_block=ffn_block,\n",
    "                         dropout=0.1, verbose=True)\n",
    "\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "\n",
    "x = dec_block(x, x, src_mask=None, tgt_mask=None)\n",
    "print()\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features=features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 8])\n",
      "\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Self-Attention Output shape torch.Size([3, 5, 8])\n",
      "Query shape torch.Size([3, 4, 5, 2])\n",
      "Key shape torch.Size([3, 4, 5, 2])\n",
      "Value shape torch.Size([3, 4, 5, 2])\n",
      "Attention Scores shape torch.Size([3, 4, 5, 5])\n",
      "Attention Output shape torch.Size([3, 4, 5, 2])\n",
      "MHA Final Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Cross-Attention Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "d_ff = 8\n",
    "\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "cross_attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=True)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "\n",
    "dec_block = DecoderBlock(features=embedding_dim, self_attn_block=attn_block,\n",
    "                         cross_attn_block=cross_attn_block, feed_forward_block=ffn_block,\n",
    "                         dropout=0.1, verbose=True)\n",
    "\n",
    "dec_stack = Decoder(features=embedding_dim, layers=nn.ModuleList([dec_block]*1))\n",
    "\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) \n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "\n",
    "x = dec_stack(x, x, src_mask=None, tgt_mask=None)\n",
    "print()\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Live Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([1, 1, 8])\n",
      "Prediction Shape torch.Size([1, 1, 8])\n",
      "Prediction Shape torch.Size([1, 2, 8])\n",
      "Prediction Shape torch.Size([1, 3, 8])\n",
      "Prediction Shape torch.Size([1, 4, 8])\n",
      "Prediction Shape torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "d_ff = 8\n",
    "\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=False)\n",
    "cross_attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=False)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "\n",
    "dec_block = DecoderBlock(features=embedding_dim, self_attn_block=attn_block,\n",
    "                         cross_attn_block=cross_attn_block, feed_forward_block=ffn_block,\n",
    "                         dropout=0.1, verbose=False)\n",
    "\n",
    "dec_stack = Decoder(features=embedding_dim, layers=nn.ModuleList([dec_block]*2))\n",
    "\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim)) # Start Token\n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "while x.size(1) <= 5: # Generate next 5 tokens\n",
    "    pred = dec_stack(x, x, src_mask=None, tgt_mask=None)\n",
    "    print(\"Prediction Shape\", pred.shape)\n",
    "    x = torch.cat([x, pred[:,-1:,:]], dim=1) # Concat Last Prediction Token to Input    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model) -> (batch_size, seq_len, vocab_size)\n",
    "        return torch.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "vocab_size = 10\n",
    "\n",
    "proj_layer = ProjectLayer(d_model=embedding_dim, vocab_size=vocab_size)\n",
    "x = torch.rand((batch_size,seq_len,embedding_dim))\n",
    "print(\"Input Shape\", x.shape) # (batch_size, seq_len, d_model)\n",
    "print()\n",
    "\n",
    "x = proj_layer(x)\n",
    "print(\"Output Shape\", x.shape) # (batch_size, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer](../images/transformer.png)\n",
    "\n",
    "[Source](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_embedding: InputEmbedding,\n",
    "                 tgt_embedding: InputEmbedding,\n",
    "                 src_pos_embedding: PositionalEmbedding,\n",
    "                 tgt_pos_embedding: PositionalEmbedding,\n",
    "                 project_layer: ProjectLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embedding = src_embedding\n",
    "        self.tgt_embedding = tgt_embedding\n",
    "        self.src_pos_embedding = src_pos_embedding\n",
    "        self.tgt_pos_embedding = tgt_pos_embedding\n",
    "        self.project_layer = project_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # src: (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
    "        x = self.src_embedding(src)\n",
    "        x = self.src_pos_embedding(x)\n",
    "        return self.encoder(x, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        # encoder_output: (batch_size, seq_len, d_model)\n",
    "        # src_mask: (batch_size, seq_len, seq_len)\n",
    "        # tgt: (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
    "        x = self.tgt_embedding(tgt)\n",
    "        x = self.tgt_pos_embedding(x)\n",
    "        return self.decoder(x, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # x: (batch_size, seq_len, d_model) -> (batch_size, seq_len, vocab_size)\n",
    "        return self.project_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([3, 5])\n",
      "Target Shape torch.Size([3, 5])\n",
      "\n",
      "EncoderBlock MHA Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock MHA Output shape torch.Size([3, 5, 8])\n",
      "EncoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Self-Attention Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Cross-Attention Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Self-Attention Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock Cross-Attention Output shape torch.Size([3, 5, 8])\n",
      "DecoderBlock FFN Output shape torch.Size([3, 5, 8])\n",
      "\n",
      "Output Shape torch.Size([3, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "d_ff = 8\n",
    "vocab_size = 10\n",
    "\n",
    "# Embeddings\n",
    "src_embedding = InputEmbedding(d_model=embedding_dim, vocab_size=vocab_size)\n",
    "tgt_embedding = InputEmbedding(d_model=embedding_dim, vocab_size=vocab_size)\n",
    "src_pos_embedding = PositionalEmbedding(d_model=embedding_dim, seq_len=seq_len, dropout=0.1)\n",
    "tgt_pos_embedding = PositionalEmbedding(d_model=embedding_dim, seq_len=seq_len, dropout=0.1)\n",
    "\n",
    "# Encoder\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=False)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "enc_block = EncoderBlock(features=embedding_dim, self_attn_block=attn_block, \n",
    "                         feed_foward_block=ffn_block, dropout=0.1,\n",
    "                         verbose=True)\n",
    "enc_stack = Encoder(features=embedding_dim, layers=nn.ModuleList([enc_block]*2))\n",
    "\n",
    "# Decoder\n",
    "attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=False)\n",
    "cross_attn_block = MultiHeadAttentionBlock(d_model=embedding_dim, num_heads=num_heads, dropout=0.1, verbose=False)\n",
    "ffn_block = FeedForwardBlock(d_model=embedding_dim, d_ff=d_ff, dropout=0.1)\n",
    "\n",
    "dec_block = DecoderBlock(features=embedding_dim, self_attn_block=attn_block,\n",
    "                         cross_attn_block=cross_attn_block, feed_forward_block=ffn_block,\n",
    "                         dropout=0.1, verbose=True)\n",
    "\n",
    "dec_stack = Decoder(features=embedding_dim, layers=nn.ModuleList([dec_block]*2))\n",
    "\n",
    "# Projection Layer\n",
    "proj_layer = ProjectLayer(d_model=embedding_dim, vocab_size=vocab_size)\n",
    "\n",
    "# Transformer\n",
    "transformer = Transformer(encoder=enc_stack, decoder=dec_stack,\n",
    "                          src_embedding=src_embedding, tgt_embedding=tgt_embedding,\n",
    "                          src_pos_embedding=src_pos_embedding, tgt_pos_embedding=tgt_pos_embedding,\n",
    "                          project_layer=proj_layer)\n",
    "\n",
    "enc_x = torch.randint(low=0, high=vocab_size, size=(batch_size,seq_len))\n",
    "dec_x = torch.randint(low=0, high=vocab_size, size=(batch_size,seq_len))\n",
    "print(\"Input Shape\", enc_x.shape) # (batch_size, seq_len)\n",
    "print(\"Target Shape\", dec_x.shape) # (batch_size, seq_len)\n",
    "print()\n",
    "\n",
    "enc_x = transformer.encode(enc_x, src_mask=None)\n",
    "dec_x = transformer.decode(enc_x, src_mask=None, tgt=dec_x, tgt_mask=None)\n",
    "out = transformer.project(dec_x)\n",
    "print()\n",
    "print(\"Output Shape\", out.shape) # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int,\n",
    "                      src_max_len: int, tgt_max_len: int,\n",
    "                      d_model: int = 512,\n",
    "                      num_layers: int = 6,\n",
    "                      num_heads: int = 8,\n",
    "                      d_ff: int = 2048,\n",
    "                      dropout: float = 0.1) -> Transformer:\n",
    "    \n",
    "    # Embedding Layers for Source and Target\n",
    "    src_embedding = InputEmbedding(d_model=d_model, vocab_size=src_vocab_size)\n",
    "    tgt_embedding = InputEmbedding(d_model=d_model, vocab_size=tgt_vocab_size)\n",
    "\n",
    "    # Positional Embedding Layers for Source and Target\n",
    "    src_pos_embedding = PositionalEmbedding(d_model=d_model, seq_len=src_max_len, dropout=dropout)\n",
    "    tgt_pos_embedding = PositionalEmbedding(d_model=d_model, seq_len=tgt_max_len, dropout=dropout)\n",
    "\n",
    "    # Encoder Blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        encoder_blocks.append(\n",
    "            EncoderBlock(\n",
    "                features = d_model,\n",
    "                self_attn_block = MultiHeadAttentionBlock(d_model=d_model, num_heads=num_heads, dropout=dropout),\n",
    "                feed_foward_block = FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout),\n",
    "                dropout = dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Decoder Blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(num_layers):\n",
    "        decoder_blocks.append(\n",
    "            DecoderBlock(\n",
    "                features = d_model,\n",
    "                self_attn_block = MultiHeadAttentionBlock(d_model=d_model, num_heads=num_heads, dropout=dropout),\n",
    "                cross_attn_block = MultiHeadAttentionBlock(d_model=d_model, num_heads=num_heads, dropout=dropout),\n",
    "                feed_forward_block = FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout),\n",
    "                dropout = dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Encoder and Decoder\n",
    "    encoder = Encoder(features=d_model, layers=nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(features=d_model, layers=nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Project Layer\n",
    "    projection_layer = ProjectLayer(d_model=d_model, vocab_size=tgt_vocab_size)\n",
    "\n",
    "    transformer =  Transformer(encoder=encoder, decoder=decoder,\n",
    "                       src_embedding=src_embedding, tgt_embedding=tgt_embedding,\n",
    "                       src_pos_embedding=src_pos_embedding, tgt_pos_embedding=tgt_pos_embedding,\n",
    "                       project_layer=projection_layer)\n",
    "    \n",
    "    # Initialize Weights using Xavier Initialization\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    \n",
    "    tokenizer_path = Path(config.tokenizer_file.format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        # WorldLevel Model Trainer, with vocab having words with frequency >= 2\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        Path(tokenizer_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "config = ScratchConfig()\n",
    "dataset_en_fr = load_dataset('opus_books', f'en-fr', split='train')\n",
    "print(dataset_en_fr[:4])\n",
    "tokenizer = get_or_build_tokenizer(config, dataset_en_fr, \"en\")\n",
    "print(tokenizer.encode(\"Hello World!\").ids)\n",
    "print(tokenizer.encode(\"Hello World!\").tokens)\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello World!\").ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "    # Create Upper Triangular Matrix of 1s\n",
    "    # diagonal=1, to exclude diagonal elements\n",
    "    # What is diagonal: A positive value excludes just as many diagonals above the main diagonal\n",
    "    #    If diagonal=0, then the diagonal elements will be 1\n",
    "    #    If diagonal=2, then the diagonal elements and one layer above also will be 0\n",
    "    \n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1) # or `return torch.tril(torch.ones(size,size)) == 1`\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, max_seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # self.sos_token =tokenizer_src.token_to_id(\"[SOS]\")]\n",
    "        # self.eos_token = torch.Tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype = torch.long)\n",
    "        # self.pad_token = torch.Tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype = torch.long)\n",
    "\n",
    "        self.sos_token = tokenizer_src.token_to_id(\"[SOS]\")\n",
    "        self.eos_token = tokenizer_src.token_to_id(\"[EOS]\")\n",
    "        self.pad_token = tokenizer_src.token_to_id(\"[PAD]\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        src_tgt_pair = self.ds[index]\n",
    "        # Getting the Source and Target texts\n",
    "        src_text = src_tgt_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_tgt_pair[\"translation\"][self.tgt_lang]\n",
    "        \n",
    "        # Tokenize the Source and Target texts to integer ids.\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_tokens = self.tokenizer_src.encode(tgt_text).ids\n",
    "\n",
    "        # If the length of the tokens is greater than max_seq_len, truncate the tokens\n",
    "        enc_input_tokens = enc_input_tokens[:self.max_seq_len-2] # 2 for SOS and EOS\n",
    "        dec_tokens = dec_tokens[:self.max_seq_len-1] # 1 for SOS/EOS \n",
    "            \n",
    "\n",
    "        # Add SOS and EOS tokens to the Source Tokens\n",
    "        enc_input_tokens = [self.sos_token] + enc_input_tokens + [self.eos_token]\n",
    "        dec_input_tokens = [self.sos_token] + dec_tokens # No EOS token for Input\n",
    "        dec_output_tokens = dec_tokens + [self.eos_token] # No SOS token for Target\n",
    "\n",
    "        # Padding: Pad the tokens to max_seq_len\n",
    "\n",
    "        # enc_input = torch.cat([\n",
    "        #     torch.tensor(enc_input_tokens, dtype=torch.long),\n",
    "        #     torch.tensor([self.pad_token] * self.max_seq_len - len(enc_input_tokens), dtype=torch.long)\n",
    "        # ], dim=0)\n",
    "\n",
    "        enc_input = F.pad(torch.tensor(enc_input_tokens, dtype=torch.long), \n",
    "                          (0, self.max_seq_len - len(enc_input_tokens)), # pad only on right side\n",
    "                          value=self.pad_token\n",
    "                          )\n",
    "        \n",
    "        dec_input = F.pad(torch.tensor(dec_input_tokens, dtype=torch.long), \n",
    "                    (0, self.max_seq_len - len(dec_input_tokens)), # pad only on right side\n",
    "                    value=self.pad_token\n",
    "                    )\n",
    "\n",
    "        label = F.pad(torch.tensor(dec_output_tokens, dtype=torch.long), \n",
    "                    (0, self.max_seq_len - len(dec_output_tokens)), # pad only on right side\n",
    "                    value=self.pad_token\n",
    "                    )\n",
    "\n",
    "        # 0 for padded tokens, 1 for non-padded tokens\n",
    "        enc_mask = (enc_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()\n",
    "        # 0 for padded tokens and causal tokens, 1 for remaining tokens\n",
    "        dec_mask = (dec_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(dec_input.size(0))\n",
    "\n",
    "        assert enc_input.shape[0] == self.max_seq_len\n",
    "        assert dec_input.shape[0] == self.max_seq_len\n",
    "        assert label.shape[0] == self.max_seq_len\n",
    " \n",
    "        return {\n",
    "            \"encoder_input\": enc_input, # (max_seq_len)\n",
    "            \"encoder_mask\": enc_mask, # (1, 1, max_seq_len)\n",
    "            \"decoder_input\": dec_input, # (max_seq_len)\n",
    "            \"decoder_mask\": dec_mask, # (1, 1, max_seq_len)\n",
    "\n",
    "            \"label\": label, # (max_seq_len)\n",
    "\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS Token id: 2\n",
      "EOS Token id: 3\n",
      "PAD Token id: 1\n"
     ]
    }
   ],
   "source": [
    "config = ScratchConfig()\n",
    "config.max_seq_len = 10\n",
    "\n",
    "try:\n",
    "    ds_raw = load_dataset('opus_books', f'{config.src_lang}-{config.tgt_lang}', split='train')\n",
    "except ValueError:\n",
    "    ds_raw = load_dataset('opus_books', f'{config.tgt_lang}-{config.src_lang}', split='train')\n",
    "\n",
    "tokenizer_src = get_or_build_tokenizer(config, ds_raw, config.src_lang)\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config.tgt_lang)\n",
    "\n",
    "print(\"SOS Token id:\", tokenizer_src.token_to_id(\"[SOS]\"))\n",
    "print(\"EOS Token id:\", tokenizer_src.token_to_id(\"[EOS]\"))\n",
    "print(\"PAD Token id:\", tokenizer_src.token_to_id(\"[PAD]\"))\n",
    "\n",
    "d = BiLingualDataset(ds_raw, tokenizer_src, tokenizer_tgt, config.src_lang, config.tgt_lang, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: tensor([  2,  82, 157, 774,   3,   1,   1,   1,   1,   1]) Shape: torch.Size([10])\n",
      "Encoder Mask: tensor([[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]], dtype=torch.int32) Shape: torch.Size([1, 1, 10])\n",
      "Decoder Input: tensor([  2, 273,   0,   1,   1,   1,   1,   1,   1,   1]) Shape: torch.Size([10])\n",
      "Decoder Mask: tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32) Shape: torch.Size([1, 10, 10])\n",
      "Label: tensor([273,   0,   3,   1,   1,   1,   1,   1,   1,   1]) Shape: torch.Size([10])\n",
      "Source Text: Le grand Meaulnes\n",
      "Target Text: The Wanderer\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder Input:\", d[0][\"encoder_input\"], \"Shape:\", d[0][\"encoder_input\"].shape)\n",
    "print(\"Encoder Mask:\", d[0][\"encoder_mask\"], \"Shape:\", d[0][\"encoder_mask\"].shape)\n",
    "print(\"Decoder Input:\", d[0][\"decoder_input\"], \"Shape:\", d[0][\"decoder_input\"].shape)\n",
    "print(\"Decoder Mask:\", d[0][\"decoder_mask\"], \"Shape:\", d[0][\"decoder_mask\"].shape)\n",
    "print(\"Label:\", d[0][\"label\"], \"Shape:\", d[0][\"label\"].shape)\n",
    "print(\"Source Text:\", d[0][\"src_text\"])\n",
    "print(\"Target Text:\", d[0][\"tgt_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config, verbose=False):\n",
    "    \n",
    "    try:\n",
    "        ds_raw = load_dataset('opus_books', f'{config.src_lang}-{config.tgt_lang}', split='train')\n",
    "    except ValueError:\n",
    "        ds_raw = load_dataset('opus_books', f'{config.tgt_lang}-{config.src_lang}', split='train')\n",
    "\n",
    "    src_lang = config.src_lang\n",
    "    tgt_lang = config.tgt_lang\n",
    "\n",
    "    # Build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, src_lang)\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, tgt_lang)\n",
    "\n",
    "    # Split Datasets, 90%-10%\n",
    "    train_ds_size = int(0.9* len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "\n",
    "    train_ds, val_ds = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BiLingualDataset(train_ds, tokenizer_src, tokenizer_tgt,\n",
    "                                src_lang, tgt_lang, config.max_seq_len)\n",
    "    val_ds = BiLingualDataset(val_ds, tokenizer_src, tokenizer_tgt,\n",
    "                                src_lang, tgt_lang, config.max_seq_len)\n",
    "    \n",
    "    if verbose:\n",
    "        # Getting Maximum Source and Target Sequence Lengths from the Dataset\n",
    "        max_src_len = 0\n",
    "        max_tgt_len = 0\n",
    "        for item in ds_raw:\n",
    "            max_src_len = max(max_src_len, len(tokenizer_src.encode(item[\"translation\"][src_lang]).ids))\n",
    "            max_tgt_len = max(max_tgt_len, len(tokenizer_tgt.encode(item[\"translation\"][tgt_lang]).ids))\n",
    "\n",
    "        # Print the Max Lengths\n",
    "        print(f\"Max Source Length: {max_src_len}\")\n",
    "        print(f\"Max Target Length: {max_tgt_len}\")\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    return train_dl, val_dl, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Source Length: 482\n",
      "Max Target Length: 471\n",
      "Length of Train Loader: 7149\n",
      "Length of Validation Loader: 795\n",
      "Source Vocab Size: 30000\n",
      "Target Vocab Size: 30000\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "config = ScratchConfig()\n",
    "train_dl, val_dl, tokenizer_src, tokenizer_tgt = get_ds(config, verbose=True)\n",
    "print(\"Length of Train Loader:\", len(train_dl))\n",
    "print(\"Length of Validation Loader:\", len(val_dl))\n",
    "print(\"Source Vocab Size:\", tokenizer_src.get_vocab_size())\n",
    "print(\"Target Vocab Size:\", tokenizer_tgt.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(\n",
    "        src_vocab_size=vocab_src_len, tgt_vocab_size=vocab_tgt_len,\n",
    "        src_max_len=config.max_seq_len, tgt_max_len=config.max_seq_len,\n",
    "        d_model=config.d_model, num_layers=config.num_layers,\n",
    "        num_heads=config.num_heads, d_ff=config.d_ff,\n",
    "        dropout=config.dropout\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2device(batch, device):\n",
    "    encoder_input = batch[\"encoder_input\"].to(device) # (batch_size, seq_len)\n",
    "    decoder_input = batch[\"decoder_input\"].to(device) # (batch_size, seq_len)\n",
    "    label = batch[\"label\"].to(device)\n",
    "\n",
    "    encoder_mask = batch[\"encoder_mask\"].to(device) # (batch_size, 1, 1, seq_len)\n",
    "    decoder_mask = batch[\"decoder_mask\"].to(device) # (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "    return encoder_input, decoder_input, label, encoder_mask, decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config, model, device, data_loader, epoch, optimizer, criterion, writer):\n",
    "    global global_iter\n",
    "    model.train()\n",
    "    train_iterator = tqdm(data_loader, desc=f\"Training epoch {epoch}\", total=len(data_loader))\n",
    "    total_loss = 0\n",
    "    for batch in train_iterator:\n",
    "        encoder_input, decoder_input, label, encoder_mask, decoder_mask = batch2device(batch, device)\n",
    "\n",
    "        # Forward Pass\n",
    "        enocder_output = model.encode(encoder_input, encoder_mask) # (batch_size, seq_len, d_model)\n",
    "        decoder_output = model.decode(enocder_output, encoder_mask, decoder_input, decoder_mask) # (batch_size, seq_len, d_model)\n",
    "        proj_output = model.project(decoder_output) # (batch_size, seq_len, tgt_vocab_size)\n",
    "\n",
    "        # Calculate Loss\n",
    "        # Flatten the output and label tensors\n",
    "        # prediction: (batch_size, seq_len, tgt_vocab_size) -> (batch_size*seq_len, tgt_vocab_size)\n",
    "        # label: (batch_size, seq_len) -> (batch_size*seq_len)\n",
    "        loss = criterion(proj_output.view(-1, proj_output.size(-1)), label.view(-1))\n",
    "        train_iterator.set_postfix(loss=f\"{loss.item():.3f}\")\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Tensorboard Logging\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), global_iter)\n",
    "        writer.flush()\n",
    "        global_iter += 1\n",
    "\n",
    "        # Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss /= len(data_loader)\n",
    "    writer.add_scalar(\"Loss_epoch/train\", total_loss, epoch)\n",
    "    writer.flush() \n",
    "        \n",
    "    # Saving the model\n",
    "    if epoch % 10 == 0:\n",
    "        Path(config.weights_folder).mkdir(parents=True, exist_ok=True)\n",
    "        file_name = config.epochs_save.format(epoch)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"global_iter\": global_iter,\n",
    "                \"text_gen_iter\": text_gen_iter,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss.item()                    \n",
    "            },\n",
    "            Path(config.weights_folder) / file_name\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, device, example, epoch, tokenizer_tgt, writer):\n",
    "    global text_gen_iter\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        example_dev = list(batch2device(example, device))\n",
    "        for i in range(len(example_dev)):\n",
    "            example_dev[i] = example_dev[i][:1] # Take only first example in the batch\n",
    "        encoder_input, decoder_input, label, encoder_mask, decoder_mask = example_dev\n",
    "        # Getting SOS and EOS token ids from tokenizer\n",
    "        sos_token = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "        eos_token = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
    "        max_len = encoder_input.size(1)\n",
    "        # Encoder Forward Pass\n",
    "        enocder_output = model.encode(encoder_input, encoder_mask)\n",
    "        # Decoder Forward Pass\n",
    "        decoder_input = torch.tensor([[sos_token]], dtype=torch.long).to(device) # (1, 1)\n",
    "        decoder_text = []\n",
    "        while True:\n",
    "            decoder_output = model.decode(enocder_output, encoder_mask, decoder_input, None)\n",
    "            proj_output = model.project(decoder_output)\n",
    "            # Get the last prediction\n",
    "            _, next_token = torch.max(proj_output[0,-1,:], dim=-1)\n",
    "            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            decoder_text.append( tokenizer_tgt.decode( [next_token.item()] ) )\n",
    "\n",
    "            if (decoder_input[0,-1] == eos_token) or (decoder_input.size(1) >= max_len):\n",
    "                break\n",
    "                \n",
    "        # Tensorboard Logging\n",
    "        writer.add_text(\"Text/Source\", example[\"src_text\"][0], text_gen_iter)\n",
    "        writer.add_text(\"Text/Target\", example[\"tgt_text\"][0], text_gen_iter)\n",
    "        writer.add_text(\"Text/Prediction\", \" \".join(decoder_text), text_gen_iter)\n",
    "        writer.flush()\n",
    "\n",
    "        text_gen_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n",
    "# example = list(batch2device(next(iter(val_dl)), \"cpu\"))\n",
    "# for i in range(len(example)):\n",
    "#     example[i] = example[i][:1] # Take only first example in the batch\n",
    "#     print(example[i].shape)\n",
    "#     # print(example[i][:1].shape)\n",
    "# generate(model, \"cpu\", next(iter(val_dl)), 1, tokenizer_tgt, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, device, val_loader, epoch, criterion, tokenizer_tgt, writer):\n",
    "    model.eval()\n",
    "    val_iterator = tqdm(val_loader, desc=f\"Validating epoch {epoch}\", total=len(val_loader))\n",
    "    random_indices = np.random.randint(0, len(val_loader), size=5)\n",
    "    total_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for idx, batch in enumerate(val_iterator):\n",
    "\n",
    "            if idx in random_indices:\n",
    "                generate(model, device, batch, epoch, tokenizer_tgt, writer)\n",
    "                \n",
    "            encoder_input, decoder_input, label, encoder_mask, decoder_mask = batch2device(batch, device)\n",
    "\n",
    "            # Forward Pass\n",
    "            enocder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(enocder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(proj_output.view(-1, proj_output.size(-1)), label.view(-1))\n",
    "            val_iterator.set_postfix(loss=f\"{loss.item():.3f}\")\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(val_loader)\n",
    "    # Tensorboard Logging\n",
    "    writer.add_scalar(\"Loss_epoch/val\", total_loss, epoch)\n",
    "    writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dl, val_dl, tokenizer_src, tokenizer_tgt= get_ds(config)\n",
    "    \n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n",
    "    model = model.to(config.device)\n",
    "    # Number of Parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    # Number of source and target Input Embedding and Position Embedding Parameters\n",
    "    src_emb_params = sum(p.numel() for p in model.src_embedding.parameters())\n",
    "    tgt_emb_params = sum(p.numel() for p in model.tgt_embedding.parameters())\n",
    "    print(\"Total Parameters:\", total_params/1e6, \"M parameters\" )\n",
    "    print(\"Source Embedding Parameters:\", src_emb_params/1e6, \"M parameters\" )\n",
    "    print(\"Target Embedding Parameters:\", tgt_emb_params/1e6, \"M parameters\" )\n",
    "    print(\"Total Embedding Parameters:\", (src_emb_params + tgt_emb_params)/1e6, \"M parameters\" )\n",
    "    print(\"Total Transformer Parameters:\", (total_params - src_emb_params - tgt_emb_params)/1e6, \"M parameters\" )\n",
    "\n",
    "    # Tensorboard Writer\n",
    "    writer = SummaryWriter(log_dir=config.log_dir, filename_suffix=f\"_{config.src_lang}_{config.tgt_lang}\")\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=1e-9)\n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id(\"[PAD]\"),\n",
    "                                    label_smoothing=config.label_smoothing) # ignore padding tokens\n",
    "    \n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        train_epoch(config, model, config.device, train_dl, epoch, optimizer, criterion, writer)\n",
    "        val_epoch(model, config.device, val_dl, epoch, criterion, tokenizer_tgt, writer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 90.250544 M parameters\n",
      "Source Embedding Parameters: 15.36 M parameters\n",
      "Target Embedding Parameters: 15.36 M parameters\n",
      "Total Embedding Parameters: 30.72 M parameters\n",
      "Total Transformer Parameters: 59.530544 M parameters\n"
     ]
    }
   ],
   "source": [
    "global_iter = 0\n",
    "text_gen_iter = 0\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss 10 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training Loss](../images/ScratchTrainLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val Loss 10 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Validation Loss](../images/ScratchValLoss.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
